{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set  up  accounts and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aparnaelangovan/PycharmProjects/venv/HiddenCut/lib/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from uuid import uuid4\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role=\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20190118T115449\".format(account_id)\n",
    "step_func_role = \"arn:aws:iam::{}:role/AmazonSageMaker-StepFunctionsWorkflowExecutionRole\".format(account_id)\n",
    "max_runs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setup image and instance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch_custom_image_name=\"ppi-extractor:gpu-1.0.0-201910130520\"\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "instance_type_gpu_map = {\"ml.p3.8xlarge\":4, \"ml.p3.2xlarge\": 1, \"ml.p3.16xlarge\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker_repo = \"{}.dkr.ecr.{}.amazonaws.com/{}\".format(account_id, region, pytorch_custom_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Configure train/ test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"aegovan-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert=\"s3://{}/embeddings/bert/\".format(bucket)\n",
    "\n",
    "# abstract_train_prefix= \"s3://aegovan-data/chemprot_adversarial/\"\n",
    "# abstract_testfile= \"s3://{}/chemprot_adversarial/chemprot_abstract_test.json\".format(bucket)\n",
    "# abstract_valfile=\"s3://{}/chemprot_adversarial/chemprot_abstract_val.json\".format(bucket)\n",
    "\n",
    "# abstract_train_prefix=\"s3://aegovan-data/chemprot_adversarial/202212041518/affable\"\n",
    "# abstract_testfile= \"s3://{}/chemprot_adversarial/202212041518/chemprot_abstract_test.json\".format(bucket)\n",
    "# abstract_valfile=\"s3://{}/chemprot_adversarial/202212041518/chemprot_abstract_val.json\".format(bucket)\n",
    "\n",
    "sst2_dataset = \"s3://aegovan-data/glue_full_set/SST-2/\"\n",
    "mnli_dataset = \"s3://aegovan-data/glue_full_set/mnli/\"\n",
    "imdb_5h_dataset = \"s3://aegovan-data/glue_full_set/imdb/imdb-5h/202306041250/00_00_00_01/\"\n",
    "\n",
    "s3_output_path= \"s3://{}/hiddencut_sagemakerresults/\".format(bucket)\n",
    "s3_code_path= \"s3://{}/hiddencut_code\".format(bucket)\n",
    "s3_checkpoint = \"s3://{}/hiddencut_bert_checkpoint/{}\".format(bucket, str(uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_localcheckpoint_dir=\"/opt/ml/checkpoints/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"imdb-json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"SST-2\" :  {\"all\": sst2_dataset},\n",
    "    \"mnli\" : {\"all\":  mnli_dataset},\n",
    "    \"imdb-json\":{\"all\":  imdb_5h_dataset},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"model_name_or_path\":\"roberta-base\" \n",
    "  , \"data_dir\": \"/opt/ml/input/data/all\"\n",
    "  , \"task_name\": dataset\n",
    "  , \"do_train\" : 1\n",
    "  , \"do_eval\" : 1\n",
    "  , \"evaluate_during_training\" :1\n",
    "  , \"do_aug\" : 1\n",
    "  , \"aug_type\" : 'attn_span_cutoff' \n",
    "  , \"aug_cutoff_ratio\" : \"0.1\"\n",
    "  , \"aug_ce_loss\": \"1.0\" \n",
    "  , \"aug_js_loss\" : \"1.0\" \n",
    "  , \"learning_rate\" : \"7e-6\" \n",
    "  , \"num_train_epochs\" : \"10\" \n",
    "  , \"logging_steps\" : \"500\"\n",
    "  , \"save_steps\" : \"500\"\n",
    "  , \"per_gpu_train_batch_size\": \"16\" \n",
    "  , \"output_dir\" : \"/opt/ml/model\"\n",
    "  , \"output_data_dir\"  : \"/opt/ml/output/data\"\n",
    "  , \"early_stop\": 100\n",
    "  , \"seed\": 42}\n",
    "\n",
    "\n",
    "temp_hyperparameters = {\n",
    "    \"model_name_or_path\":\"roberta-base\" \n",
    "  , \"data_dir\": \"/opt/ml/input/data/all\"\n",
    "  , \"task_name\": dataset\n",
    "  , \"do_train\" : 1\n",
    "  , \"do_eval\" : 1\n",
    "  , \"evaluate_during_training\" :1\n",
    "  , \"do_aug\" : 1\n",
    "  , \"aug_type\" : 'attn_span_cutoff' \n",
    "  , \"aug_cutoff_ratio\" : \"0.1\"\n",
    "  , \"aug_ce_loss\": \"1.0\" \n",
    "  , \"aug_js_loss\" : \"1.0\" \n",
    "  , \"learning_rate\" : \"7e-6\" \n",
    "  , \"num_train_epochs\" : \"1\" \n",
    "  , \"logging_steps\" : \"50\"\n",
    "  , \"save_steps\" : \"50\"\n",
    "  , \"per_gpu_train_batch_size\": \"16\" \n",
    "  , \"output_dir\" : \"/opt/ml/model\"\n",
    "  , \"output_data_dir\"  : \"/opt/ml/output/data\"\n",
    "  , \"early_stop\": 100\n",
    "  , \"seed\": 42}\n",
    "\n",
    "\n",
    "\n",
    "hyperparameters = temp_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{\"Name\": \"TrainLoss\",\n",
    "                     \"Regex\": \"###score: train_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValidationLoss\",\n",
    "                     \"Regex\": \"###score: val_loss### (\\d*[.]?\\d*)\"}\n",
    "                \n",
    "                    ,{\"Name\": \"TrainAucScore\",\n",
    "                     \"Regex\": \"###score: train_ResultScorerAucMacro_score### (\\d*[.]?\\d*)\"}\n",
    "                   ,{\"Name\": \"ValidationAucScore\",\n",
    "                     \"Regex\": \"###score: val_ResultScorerAucMacro_score### (\\d*[.]?\\d*)\"}\n",
    "                      \n",
    "                      \n",
    "                     ,{\"Name\": \"TrainF1BinaryScore\",\n",
    "                     \"Regex\": \"###score: train_ResultScorerF1Binary_score### (\\d*[.]?\\d*)\"}\n",
    "                   ,{\"Name\": \"ValidationF1BinaryScore\",\n",
    "                     \"Regex\": \"###score: val_ResultScorerF1Binary_score### (\\d*[.]?\\d*)\"}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True if you need spot instance\n",
    "use_spot = False\n",
    "train_max_run_secs =   5 *24 * 60 * 60\n",
    "spot_wait_sec =  5 * 60\n",
    "max_wait_time_secs = train_max_run_secs +  spot_wait_sec\n",
    "\n",
    "if not use_spot:\n",
    "    max_wait_time_secs = None\n",
    "    \n",
    "# During local mode, no spot.., use smaller dataset\n",
    "if instance_type == 'local':\n",
    "    use_spot = False\n",
    "    max_wait_time_secs = 0\n",
    "    wait = True\n",
    "    # Use smaller dataset to run locally\n",
    "    # TODO:\n",
    "    #  inputs = inputs_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date_fmt = datetime.datetime.today().strftime(\"%Y%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'roberta-base',\n",
       " 'data_dir': '/opt/ml/input/data/all',\n",
       " 'task_name': 'imdb-json',\n",
       " 'do_train': 1,\n",
       " 'do_eval': 1,\n",
       " 'evaluate_during_training': 1,\n",
       " 'do_aug': 1,\n",
       " 'aug_type': 'attn_span_cutoff',\n",
       " 'aug_cutoff_ratio': '0.1',\n",
       " 'aug_ce_loss': '1.0',\n",
       " 'aug_js_loss': '1.0',\n",
       " 'learning_rate': '7e-6',\n",
       " 'num_train_epochs': '1',\n",
       " 'logging_steps': '50',\n",
       " 'save_steps': '50',\n",
       " 'per_gpu_train_batch_size': '16',\n",
       " 'output_dir': '/opt/ml/model',\n",
       " 'output_data_dir': '/opt/ml/output/data',\n",
       " 'early_stop': 100,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: hiddencut-imdb-json-2023-09-10-01-35-49-599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-10 01:35:51 Starting - Starting the training job...\n",
      "2023-09-10 01:36:06 Starting - Preparing the instances for training.........\n",
      "2023-09-10 01:37:48 Downloading - Downloading input data\n",
      "2023-09-10 01:37:48 Training - Downloading the training image.........\n",
      "2023-09-10 01:39:19 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-09-10 01:39:46,539 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-09-10 01:39:46,571 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-09-10 01:39:46,573 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-09-10 01:39:46,869 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2023-09-10 01:39:46,869 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2023-09-10 01:39:46,869 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2023-09-10 01:39:46,870 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpaf03yj7d/module_dir\u001b[0m\n",
      "\u001b[34mCollecting filelock==3.0.12\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.9.1\n",
      "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.9.1->-r requirements.txt (line 2)) (3.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.9.1->-r requirements.txt (line 2)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.9.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting numpy>=1.17\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.9.1->-r requirements.txt (line 2)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.9.1->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.8.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.9.1->-r requirements.txt (line 2)) (4.56.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub==0.0.12->transformers==4.9.1->-r requirements.txt (line 2)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.9.1->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.9.1->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.9.1->-r requirements.txt (line 2)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.9.1->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.9.1->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.9.1->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.9.1->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.9.1->-r requirements.txt (line 2)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.9.1->-r requirements.txt (line 2)) (1.0.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, sacremoses\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=223986 sha256=77715abfb648c52858c4aec7fbb0179add0dd679d4b301916a43880ca0eb3b1e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_va_xhe9/wheels/dd/08/01/14fc6bdb6037371c45a5f0224c33e01628870da0e1db965544\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895254 sha256=606d22aab1a2308ba8fe030fb05e6af338d7d15f51fd8f3d16ae4c6719d72d9c\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, filelock, tokenizers, sacremoses, numpy, huggingface-hub, transformers, default-user-module-name\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 filelock-3.0.12 huggingface-hub-0.0.12 numpy-1.19.5 regex-2023.8.8 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.9.1\u001b[0m\n",
      "\u001b[34m2023-09-10 01:39:59,322 sagemaker-containers INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"all\": \"/opt/ml/input/data/all\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"aug_ce_loss\": \"1.0\",\n",
      "        \"aug_cutoff_ratio\": \"0.1\",\n",
      "        \"aug_js_loss\": \"1.0\",\n",
      "        \"aug_type\": \"attn_span_cutoff\",\n",
      "        \"data_dir\": \"/opt/ml/input/data/all\",\n",
      "        \"do_aug\": 1,\n",
      "        \"do_eval\": 1,\n",
      "        \"do_train\": 1,\n",
      "        \"early_stop\": 100,\n",
      "        \"evaluate_during_training\": 1,\n",
      "        \"learning_rate\": \"7e-6\",\n",
      "        \"logging_steps\": \"50\",\n",
      "        \"model_name_or_path\": \"roberta-base\",\n",
      "        \"num_train_epochs\": \"1\",\n",
      "        \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_gpu_train_batch_size\": \"16\",\n",
      "        \"save_steps\": \"50\",\n",
      "        \"seed\": 42,\n",
      "        \"task_name\": \"imdb-json\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"all\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"hiddencut-imdb-json-2023-09-10-01-35-49-599\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://aegovan-data/hiddencut_code/hiddencut-imdb-json-2023-09-10-01-35-49-599/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"aug_ce_loss\":\"1.0\",\"aug_cutoff_ratio\":\"0.1\",\"aug_js_loss\":\"1.0\",\"aug_type\":\"attn_span_cutoff\",\"data_dir\":\"/opt/ml/input/data/all\",\"do_aug\":1,\"do_eval\":1,\"do_train\":1,\"early_stop\":100,\"evaluate_during_training\":1,\"learning_rate\":\"7e-6\",\"logging_steps\":\"50\",\"model_name_or_path\":\"roberta-base\",\"num_train_epochs\":\"1\",\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/model\",\"per_gpu_train_batch_size\":\"16\",\"save_steps\":\"50\",\"seed\":42,\"task_name\":\"imdb-json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"all\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"all\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://aegovan-data/hiddencut_code/hiddencut-imdb-json-2023-09-10-01-35-49-599/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"all\":\"/opt/ml/input/data/all\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"aug_ce_loss\":\"1.0\",\"aug_cutoff_ratio\":\"0.1\",\"aug_js_loss\":\"1.0\",\"aug_type\":\"attn_span_cutoff\",\"data_dir\":\"/opt/ml/input/data/all\",\"do_aug\":1,\"do_eval\":1,\"do_train\":1,\"early_stop\":100,\"evaluate_during_training\":1,\"learning_rate\":\"7e-6\",\"logging_steps\":\"50\",\"model_name_or_path\":\"roberta-base\",\"num_train_epochs\":\"1\",\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/model\",\"per_gpu_train_batch_size\":\"16\",\"save_steps\":\"50\",\"seed\":42,\"task_name\":\"imdb-json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"all\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"hiddencut-imdb-json-2023-09-10-01-35-49-599\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://aegovan-data/hiddencut_code/hiddencut-imdb-json-2023-09-10-01-35-49-599/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--aug_ce_loss\",\"1.0\",\"--aug_cutoff_ratio\",\"0.1\",\"--aug_js_loss\",\"1.0\",\"--aug_type\",\"attn_span_cutoff\",\"--data_dir\",\"/opt/ml/input/data/all\",\"--do_aug\",\"1\",\"--do_eval\",\"1\",\"--do_train\",\"1\",\"--early_stop\",\"100\",\"--evaluate_during_training\",\"1\",\"--learning_rate\",\"7e-6\",\"--logging_steps\",\"50\",\"--model_name_or_path\",\"roberta-base\",\"--num_train_epochs\",\"1\",\"--output_data_dir\",\"/opt/ml/output/data\",\"--output_dir\",\"/opt/ml/model\",\"--per_gpu_train_batch_size\",\"16\",\"--save_steps\",\"50\",\"--seed\",\"42\",\"--task_name\",\"imdb-json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ALL=/opt/ml/input/data/all\u001b[0m\n",
      "\u001b[34mSM_HP_AUG_CE_LOSS=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_AUG_CUTOFF_RATIO=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_AUG_JS_LOSS=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_AUG_TYPE=attn_span_cutoff\u001b[0m\n",
      "\u001b[34mSM_HP_DATA_DIR=/opt/ml/input/data/all\u001b[0m\n",
      "\u001b[34mSM_HP_DO_AUG=1\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=1\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=1\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOP=100\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATE_DURING_TRAINING=1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=7e-6\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=50\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=roberta-base\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_GPU_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=50\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=42\u001b[0m\n",
      "\u001b[34mSM_HP_TASK_NAME=imdb-json\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_glue.py --aug_ce_loss 1.0 --aug_cutoff_ratio 0.1 --aug_js_loss 1.0 --aug_type attn_span_cutoff --data_dir /opt/ml/input/data/all --do_aug 1 --do_eval 1 --do_train 1 --early_stop 100 --evaluate_during_training 1 --learning_rate 7e-6 --logging_steps 50 --model_name_or_path roberta-base --num_train_epochs 1 --output_data_dir /opt/ml/output/data --output_dir /opt/ml/model --per_gpu_train_batch_size 16 --save_steps 50 --seed 42 --task_name imdb-json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2023-09-10 01:40:01,132 - transformers.training_args - INFO - PyTorch: setting up devices\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,151 - root - WARNING - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,152 - root - INFO - Training/evaluation parameters TrainingArguments(output_dir='/opt/ml/model', output_data_dir='/opt/ml/output/data', overwrite_output_dir=False, do_train=True, do_eval=True, do_eval_all=False, do_predict=False, evaluate_during_training=True, do_debug=False, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=7e-06, weight_decay=0.1, adam_epsilon=1e-06, adam_betas='0.9,0.98', max_grad_norm=0.0, num_train_epochs=1.0, max_steps=-1, early_stop=100, warmup_steps=0, warmup_ratio=0.06, do_aug=True, aug_type='attn_span_cutoff', aug_ce_loss=1.0, aug_js_loss=1.0, aug_cutoff_ratio=0.1, logging_dir=None, logging_first_step=False, logging_steps=50, save_steps=50, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,237 - filelock - INFO - Lock 139843162589280 acquired on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,238 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp3x479cbf\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,338 - transformers.file_utils - INFO - storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,339 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,339 - filelock - INFO - Lock 139843162589280 released on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,340 - transformers.configuration_utils - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,340 - transformers.configuration_utils - INFO - Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,419 - filelock - INFO - Lock 139842698951648 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,420 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpwm05b8y8\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,605 - transformers.file_utils - INFO - storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,605 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,606 - filelock - INFO - Lock 139842698951648 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,688 - filelock - INFO - Lock 139842698951648 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,689 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpf2jim0g8\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,829 - transformers.file_utils - INFO - storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,830 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,830 - filelock - INFO - Lock 139842698951648 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,830 - transformers.tokenization_utils - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,830 - transformers.tokenization_utils - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,994 - transformers.configuration_utils - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:01,994 - transformers.configuration_utils - INFO - Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"imdb-json\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:02,023 - filelock - INFO - Lock 139840398774792 acquired on /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:02,024 - transformers.file_utils - INFO - https://cdn.huggingface.co/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpqmh3jbfr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2023-09-10 01:40:10,575 - transformers.file_utils - INFO - storing https://cdn.huggingface.co/roberta-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:10,575 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:10,575 - filelock - INFO - Lock 139840398774792 released on /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:10,576 - transformers.modeling_utils - INFO - loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,011 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,071 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,130 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,190 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,249 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,308 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,368 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,427 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,487 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,547 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,606 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:11,666 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:14,654 - transformers.modeling_utils - INFO - Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:14,655 - transformers.modeling_utils - INFO - Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:14,733 - transformers.configuration_utils - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:14,733 - transformers.configuration_utils - INFO - Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"imdb-json\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:14,752 - transformers.modeling_utils - INFO - loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,186 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,245 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,314 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,374 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,433 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,493 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,552 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,611 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,670 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,730 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,791 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:15,850 - transformers.modeling_bert - INFO - Instantizating BertOutput from custom module\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:18,857 - transformers.modeling_utils - INFO - Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:18,857 - transformers.modeling_utils - INFO - Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\u001b[0m\n",
      "\u001b[34m125237762\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:18,904 - filelock - INFO - Lock 139840403830096 acquired on /opt/ml/input/data/all/cached_train_RobertaTokenizer_128_imdb-json.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:18,904 - transformers.data.datasets.glue - INFO - Creating features from dataset file at /opt/ml/input/data/all\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,034 - transformers.data.processors.glue - INFO - *** Example ***\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,035 - transformers.data.processors.glue - INFO - guid: train-0\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,035 - transformers.data.processors.glue - INFO - features: InputFeatures(input_ids=[0, 38, 33, 45, 450, 349, 8, 358, 65, 9, 8710, 9900, 29, 4133, 6, 53, 42, 16, 13, 686, 5, 275, 65, 38, 33, 450, 98, 444, 49069, 3809, 1589, 49007, 3809, 48709, 133, 527, 11, 24, 1403, 16, 1085, 780, 269, 6, 98, 38, 351, 9900, 90, 213, 6635, 88, 14, 4, 653, 817, 24, 780, 16, 5, 31158, 6, 5, 2190, 6, 5, 205, 3501, 6, 5, 3279, 1472, 9, 12073, 4, 20, 1569, 34, 10, 23333, 20889, 70, 5, 169, 149, 8, 47, 32, 95, 12144, 1782, 24, 4, 404, 5, 3035, 31158, 36, 1116, 768, 156, 30, 12585, 1003, 43, 39082, 47, 236, 3704, 198, 11, 5, 16433, 8, 22093, 4, 85, 9900, 29, 95, 6344, 328, 1648, 127, 3795, 21, 6889, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, input_ids_mask=None)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,035 - transformers.data.processors.glue - INFO - *** Example ***\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,035 - transformers.data.processors.glue - INFO - guid: train-1\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,035 - transformers.data.processors.glue - INFO - features: InputFeatures(input_ids=[0, 38, 341, 7, 657, 2494, 22, 35792, 21312, 6, 5, 13392, 1580, 29085, 113, 273, 7011, 15, 3943, 18, 40117, 7025, 4, 38, 206, 42, 21, 65, 9, 5, 275, 924, 15, 40117, 7025, 4, 1308, 964, 8, 38, 341, 7, 120, 561, 358, 273, 95, 7, 1183, 42, 311, 8, 52, 393, 2039, 41, 3238, 49069, 3809, 1589, 49007, 3809, 48709, 2387, 2674, 2048, 21, 15942, 4, 91, 21, 15652, 8, 98, 3036, 6269, 4, 38, 6640, 6371, 21312, 18, 6578, 4641, 6, 350, 4, 91, 21, 33466, 4, 38, 206, 11701, 14557, 4980, 702, 10, 205, 9231, 22048, 6, 350, 4, 1308, 2674, 7585, 58, 22, 35792, 21312, 6278, 5, 7817, 10352, 113, 8, 22, 725, 22561, 8, 35274, 35, 5, 13392, 1580, 10426, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, input_ids_mask=None)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,035 - transformers.data.processors.glue - INFO - *** Example ***\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,035 - transformers.data.processors.glue - INFO - guid: train-2\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,035 - transformers.data.processors.glue - INFO - features: InputFeatures(input_ids=[0, 83, 18181, 9, 3126, 8, 11644, 6, 99, 64, 47, 269, 224, 14, 74, 5083, 109, 2427, 7, 5, 16333, 8, 4002, 9, 42, 822, 4, 8274, 8, 977, 10734, 2403, 18, 7133, 13670, 2215, 117, 22772, 6, 358, 5120, 16, 3820, 19, 30934, 41936, 7457, 34149, 4, 20, 21737, 227, 5, 7457, 13353, 9, 22, 627, 588, 232, 113, 7, 5, 12664, 909, 8, 1104, 9, 18478, 16, 40579, 6, 2018, 201, 21545, 95, 141, 203, 55, 11577, 301, 16, 4, 20, 507, 461, 1310, 16, 67, 5500, 6, 25, 5, 1679, 8, 3940, 29195, 5, 28215, 1970, 7, 18478, 7, 946, 461, 81, 2155, 36, 8773, 234, 19239, 30171, 29, 2513, 4, 28696, 3809, 1589, 49007, 3809, 48709, 3684, 9, 5, 4476, 32, 1514, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, input_ids_mask=None)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,036 - transformers.data.processors.glue - INFO - *** Example ***\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,036 - transformers.data.processors.glue - INFO - guid: train-3\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,036 - transformers.data.processors.glue - INFO - features: InputFeatures(input_ids=[0, 38, 437, 10, 4268, 76, 12, 279, 1564, 3097, 4, 38, 439, 19, 127, 1141, 8, 316, 76, 793, 1354, 4, 166, 70, 3776, 5, 1569, 4, 20, 822, 16, 1461, 6, 33228, 6, 1769, 12, 20764, 8, 4940, 18452, 4, 20, 6197, 21, 1365, 615, 13, 10, 158, 76, 793, 7, 1407, 6, 53, 10619, 219, 615, 7, 489, 41, 4194, 2509, 4, 38, 802, 7957, 6274, 222, 10, 11415, 633, 8, 5, 1079, 9, 5, 2471, 21, 95, 2051, 4, 1308, 129, 3633, 16, 14, 5, 1287, 1422, 3880, 58, 45, 25, 2679, 25, 51, 197, 33, 57, 4, 252, 58, 12628, 6, 53, 1085, 3359, 66, 4, 374, 5, 97, 865, 6, 146, 12, 658, 6, 12111, 6, 7526, 6, 18535, 34226, 6, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, input_ids_mask=None)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,036 - transformers.data.processors.glue - INFO - *** Example ***\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,036 - transformers.data.processors.glue - INFO - guid: train-4\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,036 - transformers.data.processors.glue - INFO - features: InputFeatures(input_ids=[0, 38018, 19613, 3512, 16, 41, 2778, 6269, 1569, 14, 8806, 3501, 14, 3489, 817, 5, 1569, 47375, 29310, 906, 6, 175, 14425, 1468, 14, 16, 55, 87, 28342, 11, 110, 9304, 6269, 6, 463, 10, 1256, 205, 6197, 1135, 5, 754, 14, 63, 5, 4187, 3369, 28785, 748, 4, 29, 5, 9247, 4066, 2173, 4, 133, 65, 8, 129, 631, 38, 46405, 101, 59, 38018, 19613, 3512, 6, 7325, 14, 14, 4607, 21, 3999, 269, 956, 11, 5, 1569, 6, 4297, 6, 1594, 5, 4607, 21, 3999, 11, 5, 1569, 6, 12465, 9, 5, 6269, 5422, 74, 3999, 33, 13412, 4, 1106, 47, 33, 1166, 143, 1759, 6173, 59, 42, 1569, 6, 3463, 46962, 106, 70, 142, 5072, 70, 5, 34910, 46405, 101, 42, 822, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, input_ids_mask=None)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,109 - transformers.data.datasets.glue - INFO - Saving features into cached file /opt/ml/input/data/all/cached_train_RobertaTokenizer_128_imdb-json [took 0.072 s]\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,109 - filelock - INFO - Lock 139840403830096 released on /opt/ml/input/data/all/cached_train_RobertaTokenizer_128_imdb-json.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,109 - filelock - INFO - Lock 139843811015984 acquired on /opt/ml/input/data/all/cached_dev_RobertaTokenizer_128_imdb-json.lock\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,109 - transformers.data.datasets.glue - INFO - Creating features from dataset file at /opt/ml/input/data/all\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,249 - transformers.data.processors.glue - INFO - *** Example ***\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,249 - transformers.data.processors.glue - INFO - guid: dev-0\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,250 - transformers.data.processors.glue - INFO - features: InputFeatures(input_ids=[0, 1491, 129, 21, 42, 1569, 357, 87, 70, 5, 507, 191, 9, 289, 35, 574, 34250, 4, 125, 24, 21, 357, 87, 143, 1569, 156, 13, 1012, 38, 33, 655, 450, 328, 41552, 3809, 1589, 49007, 3809, 48709, 17629, 23, 5, 22, 14323, 5773, 113, 38, 192, 14, 129, 65, 650, 2441, 1569, 34, 156, 24, 35, 1336, 5, 2974, 3796, 312, 4104, 1619, 4, 38, 206, 24, 16, 86, 7, 712, 14, 333, 7, 132, 49069, 3809, 1589, 49007, 3809, 48709, 100, 40, 8109, 14, 5, 1461, 651, 56, 484, 924, 14, 58, 357, 87, 42, 6, 53, 38, 399, 75, 1508, 4, 38, 95, 226, 13565, 1691, 145, 441, 7, 2914, 5, 232, 9, 5, 4766, 11858, 15772, 19200, 456, 328, 2, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], token_type_ids=None, label=1, input_ids_mask=None)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,250 - transformers.data.processors.glue - INFO - *** Example ***\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,250 - transformers.data.processors.glue - INFO - guid: dev-1\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,250 - transformers.data.processors.glue - INFO - features: InputFeatures(input_ids=[0, 152, 16, 65, 9, 5, 3968, 920, 12, 13713, 4133, 655, 1412, 4, 38, 8930, 358, 86, 38, 192, 19194, 16600, 22, 36446, 6, 2067, 13, 162, 2155, 2901, 25, 5, 284, 512, 16, 6539, 409, 4, 152, 16, 10, 531, 192, 114, 47, 657, 3122, 328, 2700, 13131, 7989, 328, 20, 2301, 11, 5, 1569, 32, 2128, 12103, 4, 2011, 77, 208, 27633, 161, 7, 6101, 131, 22, 34440, 18, 16654, 8, 3678, 13868, 1168, 2901, 17217, 101, 42, 38, 115, 109, 396, 6, 53, 77, 38, 21, 411, 38, 5673, 38, 2638, 14, 516, 4, 20, 19879, 189, 2045, 9337, 219, 7, 103, 6, 53, 38, 101, 24, 4, 19194, 25, 5, 2530, 2335, 54, 18, 4568, 6101, 7, 185, 81, 13, 123, 77, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, input_ids_mask=None)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,250 - transformers.data.processors.glue - INFO - *** Example ***\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,250 - transformers.data.processors.glue - INFO - guid: dev-2\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,250 - transformers.data.processors.glue - INFO - features: InputFeatures(input_ids=[0, 38, 437, 10, 1307, 2378, 9, 5, 211, 23369, 9, 289, 7706, 1120, 1012, 311, 4, 178, 38, 269, 3776, 42, 22154, 4, 38, 3776, 2185, 259, 10, 319, 55, 87, 38, 222, 19, 97, 1035, 1803, 32353, 49069, 3809, 1589, 49007, 3809, 48709, 243, 18, 6269, 1576, 82, 5569, 136, 42, 1569, 19, 19791, 101, 22, 462, 4344, 6197, 113, 8, 22, 405, 18, 203, 3977, 34612, 87, 5, 311, 72, 8901, 16005, 16646, 2145, 5, 3977, 1906, 14186, 9, 5, 12073, 11, 5, 4792, 3238, 116, 25246, 817, 40112, 11248, 8, 3542, 161, 14, 5790, 56, 1153, 5886, 16375, 457, 5, 1159, 11, 5, 21297, 1580, 4, 20, 129, 1219, 24, 21, 17317, 62, 16, 142, 24, 1714, 7, 8, 656, 86, 8534, 49069, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, input_ids_mask=None)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,250 - transformers.data.processors.glue - INFO - *** Example ***\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,250 - transformers.data.processors.glue - INFO - guid: dev-3\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,251 - transformers.data.processors.glue - INFO - features: InputFeatures(input_ids=[0, 32, 5586, 9, 42, 27732, 1905, 4, 20, 13170, 9, 5, 39849, 3026, 5, 527, 9, 10, 664, 1816, 54, 3374, 26502, 1855, 31916, 1851, 71, 69, 1150, 8524, 4, 374, 41, 2946, 160, 5, 3673, 9, 3430, 6, 5, 17603, 697, 5, 2007, 1074, 9, 22, 506, 13761, 13134, 72, 1308, 5375, 16, 182, 2933, 8, 2128, 543, 7, 1166, 6, 53, 5, 822, 10902, 103, 5835, 6444, 26815, 6, 8, 5, 2280, 173, 15, 8934, 16, 35803, 4, 21038, 3637, 858, 710, 3660, 8184, 1891, 11, 42, 16219, 822, 4, 8184, 1891, 21, 416, 10, 538, 999, 11, 27732, 6, 8, 42, 822, 1302, 7, 33, 57, 1982, 95, 13, 69, 35, 2968, 19873, 664, 693, 28121, 81, 38779, 4, 8184, 1891, 21285, 784, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, input_ids_mask=None)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,251 - transformers.data.processors.glue - INFO - *** Example ***\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,251 - transformers.data.processors.glue - INFO - guid: dev-4\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,251 - transformers.data.processors.glue - INFO - features: InputFeatures(input_ids=[0, 5143, 6, 172, 6, 37, 18, 3668, 6967, 4, 125, 47, 531, 28, 12038, 8, 2119, 7, 1346, 39, 12073, 4, 91, 4865, 36, 25764, 16276, 70, 9477, 9, 7614, 6, 215, 25, 5, 78, 6950, 6684, 6, 9274, 73, 27298, 6, 6214, 13493, 30619, 6, 4133, 6, 8, 1405, 338, 211, 1638, 10321, 91, 757, 9182, 49069, 3809, 1589, 49007, 3809, 48709, 2709, 167, 9, 47, 32, 10, 15189, 7, 21854, 6, 42, 965, 75, 13, 47, 4, 616, 103, 9, 201, 15760, 21269, 19, 7034, 281, 8, 5788, 6, 37, 2939, 5, 856, 12, 14742, 4, 1578, 6, 114, 47, 64, 75, 7923, 23, 2512, 6, 393, 1183, 42, 131, 47, 40, 619, 5, 17275, 49069, 3809, 1589, 49007, 3809, 48709, 34366, 24118, 6, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, input_ids_mask=None)\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,262 - transformers.data.datasets.glue - INFO - Saving features into cached file /opt/ml/input/data/all/cached_dev_RobertaTokenizer_128_imdb-json [took 0.011 s]\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:20,262 - filelock - INFO - Lock 139843811015984 released on /opt/ml/input/data/all/cached_dev_RobertaTokenizer_128_imdb-json.lock\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2023-09-10 01:40:22,195 - root - WARNING - You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:22,195 - root - INFO - You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:22,198 - root - INFO - ***** Running training *****\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:22,198 - root - INFO -   Num examples = 500\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:22,198 - root - INFO -   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:22,198 - root - INFO -   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:22,199 - root - INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:22,199 - root - INFO -   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:22,199 - root - INFO -   Total optimization steps = 32\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:38,730 - root - INFO - ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:38,731 - root - INFO -   Num examples = 75\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:38,731 - root - INFO -   Batch size = 64\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:38,912 - root - INFO - {\"eval_loss\": 0.6916621788342794, \"eval_acc\": 0.49333333333333335, \"epoch\": 1.0, \"step\": 32}\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:38,912 - root - INFO - Saving model checkpoint to /opt/ml/model/checkpoint-32\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:40,549 - root - INFO - \u001b[0m\n",
      "\u001b[34mTraining completed.\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:40,549 - utils - INFO - \u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:40,550 - utils - INFO -         step  eval_acc eval_loss\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:40,550 - utils - INFO - --------------------------------\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:40,550 - utils - INFO -           32    0.4933    0.6917\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:40,550 - utils - INFO - --------------------------------\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:40,550 - utils - INFO -     best: 32    0.4933    0.6917\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:40,550 - root - INFO - ***** Eval results to file /opt/ml/output/data*****\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:40,550 - utils - INFO - Writing report_results_file to /opt/ml/output/data/eval_results.json\u001b[0m\n",
      "\u001b[34m[{'step': 32, 'eval_acc': 0.49333333333333335, 'eval_loss': 0.6916621788342794}]\u001b[0m\n",
      "\u001b[34m2023-09-10 01:40:41,048 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-09-10 01:40:49 Uploading - Uploading generated training model\n",
      "2023-09-10 01:42:20 Completed - Training job completed\n",
      "Training seconds: 292\n",
      "Billable seconds: 292\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "estimator = PyTorch(\n",
    "  entry_point='run_glue.py',\n",
    "                    source_dir = '../src',\n",
    "                    dependencies =['../src/transformers'],\n",
    "#                     git_config= git_config,\n",
    "#                     image_name= docker_repo,\n",
    "                    role=role,\n",
    "                    framework_version =\"1.4.0\",\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "                    hyperparameters = hyperparameters,\n",
    "                    output_path=s3_output_path,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    volume_size=30,\n",
    "                    code_location=s3_code_path,\n",
    "                    debugger_hook_config=False,\n",
    "                    base_job_name = f\"hiddencut-{dataset}\",  \n",
    "                    max_run =  train_max_run_secs,\n",
    "                    max_wait = max_wait_time_secs,   \n",
    ")\n",
    "\n",
    "\n",
    "estimator.fit(inputs[dataset], wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
